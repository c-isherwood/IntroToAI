{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1. Suppose that the performance measure is concerned with just the first T \n",
    "   time steps of the environment and ignores everything thereafter. Show that \n",
    "   a rational agentâ€™s action may depend not just on the state of the environment \n",
    "   but also on the time step it has reached.\n",
    "\n",
    " In cases where the performance measure considers a finite number of steps, the rational action depends not only on the current state but also on the time step. This dependency arises because the remaining number of steps alters the set of possible future rewards. The agent must account for the temporal proximity to the end of the decision horizon to act rationally, optimizing its strategy based on both its current state and how much time it has left to achieve its goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2. Let us examine the rationality of various vacuum-cleaner agent functions.\n",
    "1. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed rational under the assumptions listed on page \n",
    "2. Describe a rational agent function for the case in which each movement costs one point. Does the corresponding agent program require internal state?\n",
    "The program must know the current state because it needs to know what is clean/if there is more to clean to optimize efficiency. A variable would be needed to measure the threshold of what should be cleaned. \n",
    "3. Discuss possible agent designs for the cases in which clean squares can become dirty and the geography of the environment is unknown. Does it make sense for the agent to learn from its experience in these cases? If so, what should it learn? If not, why not?\n",
    "The agent would need a loop to continue to explore. You want to minimize the number of movements still so learning the number of iterations before dirt comes back would be helpful. Learning the environment allows for better efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "''' Implement a performance-measuring environment simulator for the vacuum-cleaner world \n",
    "depicted in Figure 2.8 and specified on page . Your implementation should be modular so \n",
    "that the sensors, actuators, and environment characteristics (size, shape, dirt placement, \n",
    "etc.) can be changed easily. (Note: for some choices of programming language and operating \n",
    "system there are already implementations in the online code repository.) \n",
    "give the agent a score, every movement has a cost 1, suck cost 1, NoOp cost 0, clean a dirty patch gives 10, \n",
    "starts with 5. Implement reflex agents and a simple model agent, score both agents for all \n",
    "possible states ''' \n",
    "\n",
    "class Environment: \n",
    "    def __init__(self, size, dirt_locations): \n",
    "        self.size = size \n",
    "        self.dirt = set(dirt_locations)\n",
    "\n",
    "    def is_dirty(self, location): \n",
    "        return location in self.dirt \n",
    "\n",
    "    def remove_dirt(self, location): \n",
    "        if location in self.dirt: \n",
    "            self.dirt.remove(location)\n",
    "\n",
    "    def add_dirt(self, location): \n",
    "        self.dirt.add(location)\n",
    "\n",
    "class Agent: \n",
    "    def __init__(self, initial_score=5): \n",
    "        self.location = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflex Agent Score: 10\n",
      "Model Based Agent Score: 0\n"
     ]
    }
   ],
   "source": [
    "# simple agent code \n",
    "class Environment:\n",
    "    def __init__(self, layout):\n",
    "        self.layout = layout  # layout is a dictionary with positions as keys and dirt status as values\n",
    "\n",
    "    def is_dirty(self, position):\n",
    "        return self.layout.get(position, False)\n",
    "\n",
    "    def clean(self, position):\n",
    "        if self.is_dirty(position):\n",
    "            self.layout[position] = False\n",
    "\n",
    "    def add_dirt(self, position):\n",
    "        self.layout[position] = True\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.position = 'A'  # initial position\n",
    "        self.score = 5       # initial score\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.is_dirty(self.position)\n",
    "\n",
    "    def move(self, target):\n",
    "        self.position = target\n",
    "        self.score -= 1\n",
    "\n",
    "    def suck(self):\n",
    "        if self.perceive():\n",
    "            self.environment.clean(self.position)\n",
    "            self.score += 9  # net gain of 10 for cleaning but -1 for the action\n",
    "\n",
    "    def no_op(self):\n",
    "        pass  # no operation, does nothing\n",
    "\n",
    "\n",
    "class ReflexAgent(Agent):\n",
    "    def choose_action(self):\n",
    "        if self.perceive():\n",
    "            self.suck()\n",
    "        else:\n",
    "            self.move('B' if self.position == 'A' else 'A')\n",
    "\n",
    "\n",
    "class ModelBasedAgent(Agent):\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        self.model = {'A': False, 'B': False}  # Simplified model of the world\n",
    "\n",
    "    def update_model(self):\n",
    "        self.model[self.position] = self.perceive()\n",
    "\n",
    "    def choose_action(self):\n",
    "        self.update_model()\n",
    "        if self.model[self.position]:\n",
    "            self.suck()\n",
    "        else:\n",
    "            self.move('B' if self.position == 'A' else 'A')\n",
    "\n",
    "\n",
    "class Simulation:\n",
    "    def __init__(self, agent, steps=10):\n",
    "        self.agent = agent\n",
    "        self.steps = steps\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.steps):\n",
    "            self.agent.choose_action()\n",
    "\n",
    "        return self.agent.score\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "environment = Environment({'A': True, 'B': False})\n",
    "reflex_agent = ReflexAgent(environment)\n",
    "model_agent = ModelBasedAgent(environment)\n",
    "\n",
    "simulation_reflex = Simulation(reflex_agent, 5)\n",
    "simulation_model = Simulation(model_agent, 5)\n",
    "\n",
    "print(\"Reflex Agent Score:\", simulation_reflex.run())\n",
    "print(\"Model Based Agent Score:\", simulation_model.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Dirt at A=False, Dirt at B=False, Initial Position=A -> Score: 4\n",
      "Config: Dirt at A=False, Dirt at B=False, Initial Position=B -> Score: 4\n",
      "Config: Dirt at A=False, Dirt at B=True, Initial Position=A -> Score: 13\n",
      "Config: Dirt at A=False, Dirt at B=True, Initial Position=B -> Score: 14\n",
      "Config: Dirt at A=True, Dirt at B=False, Initial Position=A -> Score: 14\n",
      "Config: Dirt at A=True, Dirt at B=False, Initial Position=B -> Score: 13\n",
      "Config: Dirt at A=True, Dirt at B=True, Initial Position=A -> Score: 22\n",
      "Config: Dirt at A=True, Dirt at B=True, Initial Position=B -> Score: 22\n",
      "Average Score: 13.25\n"
     ]
    }
   ],
   "source": [
    "# ex 12 \n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, dirt_a, dirt_b, initial_position):\n",
    "        self.positions = {'A': dirt_a, 'B': dirt_b}\n",
    "        self.agent_position = initial_position\n",
    "\n",
    "    def is_dirty(self):\n",
    "        return self.positions[self.agent_position]\n",
    "\n",
    "    def clean(self):\n",
    "        if self.is_dirty():\n",
    "            self.positions[self.agent_position] = False\n",
    "\n",
    "    def move(self):\n",
    "        self.agent_position = 'B' if self.agent_position == 'A' else 'A'\n",
    "\n",
    "\n",
    "class ReflexAgent:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.score = 5\n",
    "\n",
    "    def act(self):\n",
    "        if self.environment.is_dirty():\n",
    "            self.environment.clean()\n",
    "            self.score += 9  # 10 for cleaning, -1 for action\n",
    "        else:\n",
    "            self.environment.move()\n",
    "            self.score -= 1  # -1 for movement\n",
    "\n",
    "\n",
    "def simulate_environment(dirt_a, dirt_b, initial_position):\n",
    "    environment = Environment(dirt_a, dirt_b, initial_position)\n",
    "    agent = ReflexAgent(environment)\n",
    "    \n",
    "    # Run until no more dirt is present or a maximum of 4 actions to avoid infinite loops\n",
    "    for _ in range(4):\n",
    "        agent.act()\n",
    "        if not any(environment.positions.values()):\n",
    "            break\n",
    "    \n",
    "    return agent.score\n",
    "\n",
    "\n",
    "# Possible states are combinations of dirt at A and B and initial position\n",
    "dirt_configurations = [(False, False), (False, True), (True, False), (True, True)]\n",
    "initial_positions = ['A', 'B']\n",
    "results = {}\n",
    "\n",
    "for config in dirt_configurations:\n",
    "    for position in initial_positions:\n",
    "        score = simulate_environment(config[0], config[1], position)\n",
    "        results[(config, position)] = score\n",
    "\n",
    "# Calculate the average score\n",
    "average_score = sum(results.values()) / len(results)\n",
    "\n",
    "# Display results\n",
    "for k, v in results.items():\n",
    "    print(f\"Config: Dirt at A={k[0][0]}, Dirt at B={k[0][1]}, Initial Position={k[1]} -> Score: {v}\")\n",
    "\n",
    "print(\"Average Score:\", average_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
