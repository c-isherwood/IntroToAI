{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1. Suppose that the performance measure is concerned with just the first T \n",
    "   time steps of the environment and ignores everything thereafter. Show that \n",
    "   a rational agentâ€™s action may depend not just on the state of the environment \n",
    "   but also on the time step it has reached.\n",
    "\n",
    " In cases where the performance measure considers a finite number of steps, the rational action depends not only on the current state but also on the time step. This dependency arises because the remaining number of steps alters the set of possible future rewards. The agent must account for the temporal proximity to the end of the decision horizon to act rationally, optimizing its strategy based on both its current state and how much time it has left to achieve its goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2. Let us examine the rationality of various vacuum-cleaner agent functions.\n",
    "1. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed rational under the assumptions listed on page \n",
    "2. Describe a rational agent function for the case in which each movement costs one point. Does the corresponding agent program require internal state?\n",
    "The program must know the current state because it needs to know what is clean/if there is more to clean to optimize efficiency. A variable would be needed to measure the threshold of what should be cleaned. \n",
    "3. Discuss possible agent designs for the cases in which clean squares can become dirty and the geography of the environment is unknown. Does it make sense for the agent to learn from its experience in these cases? If so, what should it learn? If not, why not?\n",
    "The agent would need a loop to continue to explore. You want to minimize the number of movements still so learning the number of iterations before dirt comes back would be helpful. Learning the environment allows for better efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "''' Implement a performance-measuring environment simulator for the vacuum-cleaner world \n",
    "depicted in Figure 2.8 and specified on page . Your implementation should be modular so \n",
    "that the sensors, actuators, and environment characteristics (size, shape, dirt placement, \n",
    "etc.) can be changed easily. (Note: for some choices of programming language and operating \n",
    "system there are already implementations in the online code repository.) ''' \n",
    "\n",
    "print (\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
